{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 5 - Data Acquisition, Web Scraping and Web APIs *\n",
    "---\n",
    "* Some material on web scraping and usage of APIs adapted from Kevin Markham's data science courses at https://github.com/justmarkham\n",
    "\n",
    "### Content\n",
    "\n",
    "1. Data gathering via web scraping\n",
    "2. HTML basics\n",
    "3. Data gathering via web APIs\n",
    "4. JSON file format\n",
    "\n",
    "\n",
    "### Learning Outcomes\n",
    "\n",
    "At the end of this lecture, you should be able to:\n",
    "\n",
    "* list the different dynamic sources of data\n",
    "* explain what HTML is and its basic structure\n",
    "* make HTTP requests using python\n",
    "* traverse the HTML document tree\n",
    "* perform web scraping at an introductory level\n",
    "* describe and process the JSON file format\n",
    "* perform rudimentary data acquisition using Web APIs\n",
    "\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Acquisition\n",
    "\n",
    "So far, we have looked at how we can acquire data from pre-prepared Excel and text files in the CSV format. We also saw how we can use pandas clipboard facility to paste and build data frames. \n",
    "\n",
    "We also experienced that much of the data does not come in tidy formats that are prepared and ready for data analysis. For this we learned a number of techniques that help us to wrangle and tidy our data into shape. \n",
    "\n",
    "Now we are going to look at two additional sources of data that are dynamic and will require the combination of all the techniques we learned previously, such as wrangling, merging, aggregation, as well as some new skills. \n",
    "\n",
    "It is becoming common these days that data is acquired from multiple sources and merged into a single dataset. The data sources that are increasingly becoming the backbone of many analytics and information systems are web based.\n",
    "\n",
    "This section considers how data can be read (scraped) from web pages (HTML documents), and how data can be retrieved from web servers using their application program interfaces (APIs)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Web scraping\n",
    "\n",
    "Often when we need to acquire data, web pages are a great resource to turn to. \n",
    "\n",
    "The term \"web scraping\" refers to an application or script that processes HTML pages. This is done in order to extract data embedded in HTML for manipulation. \n",
    "\n",
    "Web scraping applications in effect simulate a person viewing a website with a browser.\n",
    "\n",
    "Our task then becomes writing scripts that can traverse the structure of HTML documents and locate the particular piece of data we need."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HTML\n",
    "\n",
    "Many websites make data available on their web pages for viewing in a browser, but do not make it conveniently \n",
    "downloadable as an easily machine-readable format like JSON, CSV, or XML. \n",
    "\n",
    "#### What is HTML?\n",
    "\n",
    "HTML is a markup language (not a programming language) for describing web documents (web pages).\n",
    "\n",
    "    HTML stands for Hyper Text Markup Language\n",
    "    A markup language is a set of markup tags\n",
    "    HTML documents are described by HTML tags\n",
    "    Each HTML tag describes different document content\n",
    "\n",
    "HTML pages consist of elements. Elements are marked up by tags and may have attributes inside them which describe how the content should be rendered by web browsers.\n",
    "\n",
    "Please refer to http://www.w3schools.com/html/html_intro.asp for an introduction to HTML."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "<!DOCTYPE html>\n",
    "<html>\n",
    "  <head>\n",
    "    <title>This is a title</title>\n",
    "  </head>\n",
    "  <body >\n",
    "    <p>Hello world!</p>\n",
    "  </body>\n",
    "</html>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The examples below will show how we can perform web scraping on HTML pages using a Python package called `BeautifulSoup`. \n",
    "\n",
    "BeautifulSoup is an HTML/XML parser for Python that can turn markup text into a parse tree, that can then be traversed more easily."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from IPython.core.display import HTML\n",
    "HTML(\"<iframe src=http://www.crummy.com/software/BeautifulSoup/bs4/doc/ width=1100 height=500></iframe>\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BeautifulSoup provides a simplified, idiomatic ways of navigating, searching, and modifying parse tree generated by HTML and XML.\n",
    "\n",
    "More info on BeautifulSoup http://www.crummy.com/software/BeautifulSoup/bs3/documentation.html\n",
    "\n",
    "Good examples of how this is done can be found in : http://www.gregreda.com/2013/03/03/web-scraping-101-with-python/ and http://blog.miguelgrinberg.com/post/easy-web-scraping-with-python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intro to Web Scraping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to begin with a toy example first using the simple html page created below:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "<!doctype html>\n",
    "<html lang=\"en\">\n",
    "<head>\n",
    "  <title>Teo's Webpage</title>\n",
    "</head>\n",
    "\n",
    "<body>\n",
    "  <h1>Teo's Webpage</h1>\n",
    "  <p id=\"intro\">My name is Teo.  I find web scraping interesting.</p>\n",
    "  <p id=\"background\">I live in Auckland and completed my PhD at Massey University in Computer Science, while studying the field of machine learning.</p>\n",
    "  <p id=\"current\">I currently work as a lecturer in Information Technology.</p>\n",
    "  \n",
    "  <h3>My Interests</h3>\n",
    "  <ul>\n",
    "      <li id=\"my favorite\">Data Science and Machine Learning</li>\n",
    "      <li class=\"hobby\">Tennis</li>\n",
    "      <li class=\"hobby\">Reading</li>\n",
    "      <li class=\"hobby\">Travelling</li>\n",
    "      <li class=\"hobby\">Reading</li>\n",
    "  </ul>\n",
    "</body>\n",
    "</html>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# imports\n",
    "import requests                 # How Python gets the webpages\n",
    "from bs4 import BeautifulSoup   # Creates structured, searchable object\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# First, let's read the toy webpage as a string - this is what happens initially when you scrape any webpage\n",
    "html_doc = \"\"\"\n",
    "<!doctype html>\n",
    "<html lang=\"en\">\n",
    "<head>\n",
    "  <title>Teo's Webpage</title>\n",
    "</head>\n",
    "\n",
    "<body>\n",
    "  <h1>Teo's Webpage</h1>\n",
    "  <p id=\"intro\">My name is Teo.  I find web scraping interesting.</p>\n",
    "  <p id=\"background\">I live in Auckland and completed my PhD at Massey University in Computer Science, while studying the field of machine learning.</p>\n",
    "  <p id=\"current\">I currently work as a lecturer in Information Technology.</p>\n",
    "  \n",
    "  <h3>My Interests</h3>\n",
    "  <ul>\n",
    "      <li id=\"my favorite\">Data Science and Machine Learning</li>\n",
    "      <li class=\"hobby\">Tennis</li>\n",
    "      <li class=\"hobby\">Reading</li>\n",
    "      <li class=\"hobby\">Travelling</li>\n",
    "      <li class=\"hobby\">Reading</li>\n",
    "  </ul>\n",
    "</body>\n",
    "</html>\n",
    "\"\"\"\n",
    "type(html_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Beautiful soup allows us to create structure from the html elements, and to traverse it\n",
    "page = BeautifulSoup(html_doc)\n",
    "print type(page)\n",
    "page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# The most useful methods in a Beautiful Soup object are \"find\" and \"findAll\".\n",
    "# \"find\" takes several parameters, the most important are \"name\" and \"attrs\".\n",
    "# name will help us find the type of an element\n",
    "# Let's target \"name\".\n",
    "page.find(name='body') # Finds the 'body' tag and everything inside of it.\n",
    "body = page.find(name='body')\n",
    "type(body) #element.Tag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above result tells us that 'body' element was found in the HTML page, and it tells us what object type it is. We can see its content below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "body.contents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can recursively search for other elements inside the returned result as well:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "h1 = body.find(name='h1') # Find the 'h1' element inside of the 'body' tag\n",
    "print h1\n",
    "print h1.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how we can access the entire element or just the content. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's find the 'p' elements:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "p = page.find(name='p')\n",
    "# This only finds one.  This is where 'findAll' comes in.\n",
    "print p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "all_p = page.findAll(name='p')\n",
    "print all_p\n",
    "print type(all_p) # Result sets are a lot like Python lists"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Access specific element with index:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print all_p[0]\n",
    "print all_p[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Iterable like  list\n",
    "for one_p in all_p:\n",
    "    print one_p.text # Print text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Access specific attribute of a tag:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print all_p[0] # Specific element\n",
    "print all_p[0]['id'] # Specific attribute value of a specific element"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's look at 'attrs'. Beautiful soup also allows us to locate elements with specific attributes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print page.find(name='p', attrs={\"id\":\"intro\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print page.find(name='p', attrs={\"id\":\"background\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "result = page.find(name='p', attrs={\"id\":\"current\"})\n",
    "result.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also do a search of all instances of an element and name of a class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print page.findAll(\"li\", \"hobby\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** Extract the 'h3' element from Teo's webpage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "page.find(name='h3')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** Extract Teo's hobbies from the html_doc.  Print out the text of the hobby. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "hobbies = page.findAll(name='ul')\n",
    "for hobby in hobbies:\n",
    "    print hobby.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** Extract Teo's hobby that has the id \"my favorite\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "page.find(name='li', attrs={'id':'my favorite'}).text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to illustrate HTML web scraping on a real-world site, we will look at a website that lists the up-to-date gold price found on http://www.gold.org, and which is refreshed every minute. \n",
    "\n",
    "We will attempt to read the asking price of gold from the HTML document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from IPython.core.display import HTML\n",
    "HTML(\"<iframe src=http://www.gold.org width=1100 height=500></iframe>\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The price we are interested in is found in the \"ASK\" row under the \"Spot Price\" section. \n",
    "\n",
    "In order to find where the price is situated in the HTML document, we must look at the document's source code. By right clicking on a page in a browser, an option should be displayed allowing you to view the source.\n",
    "\n",
    "We must inspect the source so that we can find the element that houses this value. We can then use the python's BeautifulSoup package to read and iterate through the HTML elements in order to extract the data that we want.\n",
    "\n",
    "There are three basic steps to scraping a single page:\n",
    "\n",
    "    1. Get (request) the page\n",
    "    2. Parse the page content (read and interpret the document structure)\n",
    "    3. Search through the content of interest\n",
    "\n",
    "\n",
    "Below is the example of a script that will access and display the latest gold price being traded:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#we first need to make some extra imports\n",
    "import json\n",
    "from time import sleep\n",
    "from datetime import datetime\n",
    "\n",
    "#you might need to set the proxies if you are doiung this from Massey's domain\n",
    "#if the below does not work, then try this: \"http://get-proxy.massey.ac.nz/\"\n",
    "massey_proxies = {\n",
    "  \"http\": \"http://alb-cache1.massey.ac.nz/\",\n",
    " \"https\": \"http://alb-cache1.massey.ac.nz/\",\n",
    "}\n",
    "\n",
    "#massey_proxies = {\n",
    " # \"http\": \"http://get-proxy.massey.ac.nz/\",\n",
    "#  \"https\": \"http://get-proxy.massey.ac.nz/\",\n",
    "#}\n",
    "\n",
    "#massey_proxies = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**STEP 1: GET** Access the page and read it into the beautiful soup object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "url = \"http://gold.org\"\n",
    "response = requests.get(url, proxies=massey_proxies)\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "page = response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "page"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**STEP 2: PARSE** Create a BeautifulSoup object that reads and parses the HTML page into a format that we can search and traverse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "scraping = BeautifulSoup(page) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "scraping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can search for a given tag, id or class name.\n",
    "\n",
    "**STEP 3: SEARCH** Search through the page for 'dd' type tags with the class name 'value':"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "element = scraping.find(\"dd\", attrs={\"class\" : \"value\"})\n",
    "element"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have found the tag we want, we extract the contents of it by calling .contents and optionally convert it into a float."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print float(str(element.contents[0]).replace(',', ''))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As it turns out, there are multiple tags in the document with this tag-name combination. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we re-run the search from before and ask for all results to be returned that match our criteria, this is what we get:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "element = scraping.find_all(\"dd\", attrs={\"class\" : \"value\"})\n",
    "element"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "ATTRIBUTES\n",
    "\n",
    "Classes are NOT unique\n",
    "\n",
    "    You can use the same class on multiple elements.\n",
    "    You can use multiple classes on the same element.\n",
    "\n",
    "ID's are unique and elements with ID's are easy to extract\n",
    "\n",
    "    Each element can have only one ID\n",
    "    Each page can have only one element with that ID\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our previous scrape worked because the value of interest was the first one, say we would like to scrape the mid price now (there could however be a shortcut)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "element2 = scraping.find(\"div\", attrs={\"class\" : \"asset mid\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print element2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "element2.find(\"dd\", \"value\").contents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** Scrape the bid price from the web page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#step 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "page = response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#step 2\n",
    "scraping "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "element_bid_price = scraping.find_all(attrs={\"class\" : \"asset-inner\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "element_bid_price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "element_bid_price[2].find(\"dd\", \"value\").contents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is an example of how we might write a script that continually extracts data from a page every 1-2 seconds:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def GetGoldPrice():\n",
    "    url = \"http://gold.org\"\n",
    "    response = requests.get(url, proxies=massey_proxies)\n",
    "    page = response.content\n",
    "    #create a BeautifulSoup object that reads in the HTML page\n",
    "    scraping = BeautifulSoup(page)\n",
    "    #search through the page for 'dd' type tags with the class name 'value'\n",
    "    element = scraping.find(\"dd\", \"value\")\n",
    "    #access the contents inside the tags\n",
    "    price = element.contents[0].string\n",
    "    return price\n",
    "\n",
    "for x in range(0,10):\n",
    "    time_now = datetime.now().strftime(\"%I:%M:%S%p\")\n",
    "    print(\"{0}, Gold price is: {1} \\n \".format(time_now, GetGoldPrice()))\n",
    "    sleep(0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**: Extract the current FTSE 100 stock market index from the Google Finance page http://www.google.com/finance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#step 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#step 2\n",
    "\n",
    "#scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "element_FTSE100 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "element_FTSE100.contents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also read in entire HTML tables into dataframe objects:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "scraping_html_table = BeautifulSoup(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "scraping_html_table_FTSE100 = scraping.find_all(\"table\", \"quotes\")\n",
    "scraping_html_table_FTSE100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = pd.io.html.read_html(str(scraping_html_table_FTSE100))\n",
    "df[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Web APIs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Web servers serve out web pages in the HTML format as they are requested by users. Web servers are also capable of providing data that is not formatted in HTML. These web server provide public (and private) APIs through which users can interact, construct queries that the web servers understand, and receive data from them. Depending on who owns them, web servers will have different APIs. They usually provide developer help pages that demonstrate how they work and how queries can be constructed using HTTP which the servers understand.\n",
    "\n",
    "Many websites have public APIs providing data feeds via JSON or some other format. We will consider only JSON as it is becoming a standard, and is conveniently, virtually identical to python's dictionaries in its syntax. \n",
    "\n",
    "Increasingly though, in order to access these APIs we must register for API Keys. They are credentials. Some of them are free and simply require that an account be created with a given website, while others must be purchased and have limits on the amount of data that can be pulled.\n",
    "\n",
    "There are a number of ways to access these APIs. REST is becoming the most common mechanism. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### REST\n",
    "\n",
    "REST is a lightweight mechanism built on top of the HTTP protocol which enables applications to exchange data with severs. A combination of HTTP requests, together with valid REST queries can easily be constructed from Python. One easy-to-use method is through the `requests` package (http://docs.python-requests.org).\n",
    "\n",
    "Previously, using Web Services and SOAP would result in queries like:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "<?xml version=\"1.0\"?>\n",
    "<soap:Envelope\n",
    "xmlns:soap=\"http://www.w3.org/2001/12/soap-envelope\"\n",
    "soap:encodingStyle=\"http://www.w3.org/2001/12/soap-encoding\">\n",
    " <soap:body pb=\"http://www.whitepages.com/phonebook\">\n",
    "  <pb:GetUserDetails>\n",
    "   <pb:UserID>12345678</pb:UserID>\n",
    "  </pb:GetUserDetails>\n",
    " </soap:Body>\n",
    "</soap:Envelope>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using REST, such clumsy queries can be transformed into simple HTTP requests of a format (1) like:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "http://www.whitepages.com/phonebook/UserDetails/12345678"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or alternatively, passing arguments using format (2) as follows:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "http://www.whitepages.com/phonebook?UserDetails=12345678"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are slight differences in what you can expect from the two formats. Format 1 (path segment parameter) will return a 404 error when the parameter value does not correspond to an existing resource. \n",
    "\n",
    "Format 2 uses optional parameters. Instead of en error, this format will return an empty list when the parameter is not found in the query result. \n",
    "\n",
    "Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#echonest api\n",
    "url = \"http://developer.echonest.com/api/v4/artist/reviews?api_key=YB4F9B7ZLS2YMOGUG&id=ARH6W4X1187B99274F&format=json&results=1&start=0\"\n",
    "response = requests.get(url, proxies=massey_proxies)\n",
    "\n",
    "#we want HTTP Response 200\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "response_json = response.content\n",
    "response_json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### JSON\n",
    "\n",
    "JSON (short for JavaScript Object Notation) has become one of the standard formats\n",
    "for sending data by HTTP request between web servers and browsers and other applications. \n",
    "\n",
    "It is a much more flexible data format than a tabular text form like CSV. \n",
    "\n",
    "Here is an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#In Python triple-quoted strings allow us to include strings that have escape chars in it.\n",
    "obj = \"\"\"\n",
    "{\"name\": \"Massey University\",\n",
    "\"campuses_NZ\": [\"Albany\", \"Palmerston North\", \"Wellington\"],\n",
    "\"campuses_international\": null,\n",
    "\"colleges\": [{\"name\": \"Sciences\", \"degrees\": 10, \"majors\": 30},\n",
    "{\"name\": \"Business\", \"degrees\": 8, \"majors\": 25}]\n",
    "}\n",
    "\"\"\"\n",
    "obj\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "JSON is very nearly valid Python code with the exception of its null value `null` and\n",
    "some other nuances (such as disallowing trailing commas at the end of lists). The basic\n",
    "types are objects (dicts), arrays (lists), strings, numbers, booleans, and nulls. \n",
    "\n",
    "**All of the keys in an object must be strings**. There are several Python libraries for reading and\n",
    "writing JSON data. We will use `json` here as it is built into the Python standard library. \n",
    "\n",
    "To convert (deserialize) a JSON string from above to an equivalent Python object (`dict`), use `json.loads`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "result = json.loads(obj)\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`json.dumps` on the other hand converts a Python object back to JSON:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "as_json = json.dumps(result)\n",
    "as_json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How you convert a JSON object or list of objects to a DataFrame or some other data\n",
    "structure for analysis will be up to you. Conveniently, you can pass a list of JSON objects\n",
    "to the DataFrame constructor and select a subset of the data fields:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "massey_colleges = pd.DataFrame(result['colleges'], columns=['name', 'degrees'])\n",
    "massey_colleges"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can convert a data frame back to a JSON object with the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "massey_colleges.to_json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Acquisition from APIs\n",
    "\n",
    "\n",
    "A popular API provider is https://apigee.com/providers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from IPython.core.display import HTML\n",
    "HTML(\"<iframe src=https://apigee.com/providers width=1100 height=500></iframe>\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will look at getting data from Echo Nest. The Echo Nest offers an array of music data and services for developers to build apps and experiences.\n",
    "\n",
    "Echo Nest API Console: https://apigee.com/console/echonest\n",
    "\n",
    "Echo Nest Developer Center: http://developer.echonest.com/\n",
    "\n",
    "We can use a free session API key from the service:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# request data from the Echo Nest API\n",
    "url = 'http://developer.echonest.com/api/v4/artist/top_hottt?api_key=YB4F9B7ZLS2YMOGUG&format=json'\n",
    "response = requests.get(url, proxies=massey_proxies)\n",
    "\n",
    "#we want HTTP Response 200 - not 404\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "response.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# decode JSON\n",
    "print type(response.json())\n",
    "result = response.json()\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# pretty print for easier readability\n",
    "import pprint\n",
    "pprint.pprint(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# pull out the artist data\n",
    "artists = result['response']['artists']    # list of 15 dictionaries\n",
    "artists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# reformat data into a table structure\n",
    "artists_data = [artist.values() for artist in artists]  # list of 15 lists\n",
    "artists_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "artists_header = artists[0].keys()                      # list of 2 strings\n",
    "artists_header"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "artists[0]['name']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** Have a look through the Echonest API and generate a query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## API Repositories\n",
    "\n",
    "A large number of other API repositories can be found under these links:\n",
    "\n",
    "http://www.publicapis.com/\n",
    "\n",
    "http://www.programmableweb.com/apis/directory\n",
    "\n",
    "Mashape (http://www.publicapis.com/) is the Cloud API Marketplace where developers can easily consume Cloud APIs to integrate in their next project, and where existing APIs can be distributed to the community and monetized.\n",
    "\n",
    "In order to access their APIs, it is usually required to at least create an account, while some web sites will charge fees for accessing their data. There are different ways of communicating with API servers. Mashape has created a python library that can simplify accessing their data. The library is called *unirest* and can easily be installed on your computer if you type in your command line the following line: "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "pip install unirest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import unirest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the free APIs listed under this market place is Bitcoin Exchange Rates which lists exchange rates between major companies and bitcoin as well as exchange rates between the major currencies.\n",
    "\n",
    "https://www.mashape.com/montanaflynn/bitcoin-exchange-rates#\n",
    "\n",
    "Below is an example of how to construct a query for the buying price of one bitcoin, wit hthe result returned in USD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "response = unirest.get(\"https://montanaflynn-bitcoin-exchange-rate.p.mashape.com/prices/buy?qty=1\",\n",
    "  headers={\n",
    "    \"X-Mashape-Key\": \"2BTWnoXPgrmshykB91haA2hod3UYp1FDVvyjsnjK3EfNKw5329\",\n",
    "    \"Accept\": \"text/plain\"\n",
    "  }\n",
    ")\n",
    "\n",
    "response.body"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "type(response.body)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the type of the result response.body is a familiar dictionary from which we can easily extract our data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** Extract the total amount of the cost for 1 bitcoin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "btc['total']['amount']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** Execute a query for the cost of 15 for bitcoins and extract the total price from the dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is an example of a query for extracting the current exchange rates between the major currencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "response = unirest.get(\"https://montanaflynn-bitcoin-exchange-rate.p.mashape.com/currencies/exchange_rates\",\n",
    "  headers={\n",
    "    \"X-Mashape-Key\": \"QgrDeDPRdFmshQBsi3cDAvZvD6Ykp1AxBj4jsn1po92UN8XxKx\",\n",
    "    \"Accept\": \"text/plain\"\n",
    "  }\n",
    ")\n",
    "\n",
    "response.body"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** Search through the https://www.mashape.com/montanaflynn/bitcoin-exchange-rates# webpage and find out how to construct a query to extract from their API the sell price for a single bitcoin. Execute this and extract the price."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# These code snippets use an open-source library. http://unirest.io/python\n",
    "response = unirest.get(\"https://montanaflynn-bitcoin-exchange-rate.p.mashape.com/prices/sell?qty=15\",\n",
    "  headers={\n",
    "    \"X-Mashape-Key\": \"2BTWnoXPgrmshykB91haA2hod3UYp1FDVvyjsnjK3EfNKw5329\",\n",
    "    \"Accept\": \"text/plain\"\n",
    "  }\n",
    ")\n",
    "\n",
    "response.body['total']['amount']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Markit http://www.markit.com/Company/About-Markit is a provider of financial information services.\n",
    "\n",
    "Below is an example of how the current stock proce of Apple can be queried though their API\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "url = \"http://dev.markitondemand.com/Api/v2/Quote/json?symbol=AAPL\"\n",
    "response = requests.get(url, proxies=massey_proxies)\n",
    "\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "markit_dict = json.loads(response.content)\n",
    "markit_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** Look through their API documentation at http://dev.markitondemand.com/#doc_lookup and construct a query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yahoo finance is one more source of fiancial data. Here are two different levels of detail. The first with all the detail and the second with some of it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "url = \"http://query.yahooapis.com/v1/public/yql?q=select%20*%20from%20yahoo.finance.quotes%20where%20symbol%20IN%20(%22YHOO%22,%22AAPL%22)&format=json&env=http://datatables.org/alltables.env\"\n",
    "response = requests.get(url, proxies=massey_proxies)\n",
    "\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "yahoo_fin1 = json.loads(response.content)\n",
    "yahoo_fin1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "url = \"http://finance.yahoo.com/webservice/v1/symbols/YHOO,AAPL/quote?format=json&view=detail\"\n",
    "response = requests.get(url, proxies=massey_proxies)\n",
    "\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "yahoo_fin2 = json.loads(response.content)\n",
    "yahoo_fin2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%javascript\n",
    "IPython.load_extensions('calico-spell-check')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
